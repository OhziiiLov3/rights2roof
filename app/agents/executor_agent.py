from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_openai import ChatOpenAI
from app.models.schemas import ExecutionPlan, ToolOutput, ExecutorOutput
from app.tools import geo_tools, wikipedia_tools, tavily_tools, time_tools, google_news_tool
from langsmith import traceable
from typing import List

import os
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")

#Step 1: Define Model LLM for executor agent and combined result
executor_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY, verbose=True)
combined_llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY, verbose=True)

#Step 2:
tool_parser=PydanticOutputParser(pydantic_object=ToolOutput)

#Step 3: Create Dictionary of tools
AVAILABLE_TOOLS = [geo_tools.geo_tool, wikipedia_tools.wikipedia_tool, tavily_tools.tavily_tool, time_tools.time_tool ]

tool_descriptions = [f"- {t.name}: {t.description}" for t in AVAILABLE_TOOLS]

TOOLS_MAP= {
    "wikipedia_search":wikipedia_tools.wikipedia_tool,
    "tavily_tool":tavily_tools.tavily_tool,
    "real_estate_news_update":google_news_tool.real_estate_news_tool,
}

#Step 4: Create decision system message
decision_system_message=f"""
You are an executor agent. 
Your job is to decide which tools to use based on:
1. The plan generated by the planner.
2. The retrieved knowledge (rag_result).
3. The original user query.

Available tools:
{chr(10).join(tool_descriptions)}

Rules:
- Select only tools that add value for answering the query.
- Return your decision in JSON as a list of tool calls:
  [
    {{"tool": "<tool_name>", "input": "<input string>"}},
    ...
  ]

- {tool_parser.get_format_instructions()}  
- Do not include any text outside of the JSON.
"""

#Step 5: Create decision prompt
decision_prompt=ChatPromptTemplate.from_messages([
    ("system",decision_system_message),
    ("human","User query: {query}\n\nPlan: {plan}\n\nRAG: {rag}")
])

#Step 6: Create combined system message
combined_system_message=f"""
You are a synthesizer agent.
Given the original user query and the observations (tool outputs),
produce a concise and actionable final answer.

- Be clear and to the point.
- Provide recommended next steps if relevant.
- Avoid repeating irrelevant details from the plan or RAG.
"""

#Step 7: Create combined system prompt
combined_system_prompt=ChatPromptTemplate.from_messages([
    ("system",combined_system_message),
    ("human","User query: {query}\n\nObservations: {observations}")
])

#Step 8: Create helper function to help execute tool call
def execute_tool_call(tool_call:dict)->ToolOutput:
    tool_name=tool_call.get("tool")
    tool_input=tool_call.get("input")
    tool_function=TOOLS_MAP.get(tool_name)

    if not tool_function:
        return ToolOutput(tool=tool_name,input=tool_input,output=f"Tool {tool_name} not found")
    try: 
        output=tool_function.invoke(tool_input)
        return ToolOutput(tool=tool_name,input=tool_input,output=output) 
    except Exception as error:
        return ToolOutput(tool=tool_name,input=tool_input,output=f"Error: {str(error)}")

geo_call = {"tool": "geo_location", "input": "8.8.8.8"}  # IP address for testing
geo_result = execute_tool_call(geo_call)
print("Geo Tool Output:", geo_result)

# Example 2: Test wikipedia_search
wiki_call = {"tool": "wikipedia_search", "input": "Venezuela history"}
wiki_result = execute_tool_call(wiki_call)
print("Wikipedia Tool Output:", wiki_result)


#Step X: Create function to run our agent - we will need to come back and update with RAG Agent into argument to replace "query"
def execute_agent(plan:ExecutionPlan,query:str,verbose=False)->ExecutorOutput:
    """
    runs each step in the plan:
    1) Uses LLM to select the best tool
    2) Invokes the tool
    3) Combines a final answer from the tool outputs
    """

    observations:List[ToolOutput]=[]
    return ExecutorOutput(final_answer="",observations=[])
