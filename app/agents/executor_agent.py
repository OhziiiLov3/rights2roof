from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_openai import ChatOpenAI
from app.models.schemas import ExecutionPlan, ToolOutput, ExecutorOutput, RAGAgentResponse
from app.tools import geo_tools, wikipedia_tools, tavily_tools, time_tools, google_news_tool
from langsmith import traceable
from typing import List

import os
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")

#Step 1: Define Model LLM for executor agent and combined result
executor_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY, verbose=True)
combined_llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY, verbose=True)

#Step 2:
tool_parser=PydanticOutputParser(pydantic_object=ToolOutput)

#Step 3: Create Dictionary of tools
AVAILABLE_TOOLS = [geo_tools.geo_tool, wikipedia_tools.wikipedia_tool, tavily_tools.tavily_tool, time_tools.time_tool ]

tool_descriptions = [f"- {t.name}: {t.description}" for t in AVAILABLE_TOOLS]

TOOLS_MAP= {
    "wikipedia_search":wikipedia_tools.wikipedia_tool,
    "tavily_tool":tavily_tools.tavily_tool,
    "real_estate_news_update":google_news_tool.real_estate_news_tool,
}

#Step 4: Create decision system message
decision_system_message=f"""
You are an executor agent. 
Your job is to decide which tools to use based on:
1. The plan generated by the planner.
2. The retrieved knowledge (rag_result).
3. The original user query.

Available tools:
{chr(10).join(tool_descriptions)}

Rules:
- Select only tools that add value for answering the query.
- Return your decision in JSON as a list of tool calls:
  [
    {{"tool": "<tool_name>", "input": "<input string>"}},
    ...
  ]

- {tool_parser.get_format_instructions()}  
- Do not include any text outside of the JSON.
"""

#Step 5: Create decision prompt
decision_prompt=ChatPromptTemplate.from_messages([
    ("system",decision_system_message),
    ("human","User query: {query}\n\nPlan: {plan}\n\nRAG: {rag}")
])

#Step 6: Create combined system message
combined_system_message=f"""
You are a synthesizer agent.
Given the original user query and the observations (tool outputs),
produce a concise and actionable final answer.

- Be clear and to the point.
- Provide recommended next steps if relevant.
- Avoid repeating irrelevant details from the plan or RAG.
"""

#Step 7: Create combined system prompt
combined_prompt=ChatPromptTemplate.from_messages([
    ("system",combined_system_message),
    ("human","User query: {query}\n\nObservations: {observations}")
])

#Step 8: Create helper function to help execute tool call
def execute_tool_call(tool_call:dict)->ToolOutput:
    tool_name=tool_call.get("tool")
    tool_input=tool_call.get("input")
    tool_function=TOOLS_MAP.get(tool_name)

    if not tool_function:
        return ToolOutput(tool=tool_name,input=tool_input,output=f"Tool {tool_name} not found")
    try: 
        output=tool_function.invoke(tool_input)
        return ToolOutput(tool=tool_name,input=tool_input,output=output) 
    except Exception as error:
        return ToolOutput(tool=tool_name,input=tool_input,output=f"Error: {str(error)}")


#Step X: Create function to run our agent
@traceable 
def execute_agent(rag_result:RAGAgentResponse,plan_result:ExecutionPlan,query:str,verbose=False)->ExecutorOutput:
    """
    runs each step in the plan:
    1) Uses LLM to select the best tool
    2) Invokes the tool
    3) Combines a final answer from the tool outputs
    """

#Step XX: Decide what tools to call
    try:
        decision_chain=decision_prompt | executor_llm | tool_parser 
        tools_call=decision_chain.invoke({
            "query":query,
            "plan":plan_result.model_dump(),
            "rag": rag_result.response
        })

    #Step XXX: Create observations
        observations=[execute_tool_call(tool) for tool in tools_call]

    #Step XX: Create synthesizer (last step might change)
        combined_chain = combined_prompt| combined_llm
        final_answer = combined_chain.invoke({
                "query": query,
                "observations": [obs.dict() for obs in observations]
            })

        return ExecutorOutput(final_answer=final_answer, observations=observations)

    except Exception as e:
        return ExecutorOutput(final_answer=f"Executor agent failed: {str(e)}", observations=[])

#Step XXX: Create decision chain

